{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272697f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/benjamin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/benjamin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f41e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3EBHHCZO6V2A4</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>Amaranth \"music fan\"</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>It's hard to believe \"Memory of Trees\" came ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>Enya's last great album</td>\n",
       "      <td>1158019200</td>\n",
       "      <td>09 12, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AZPWAXJG9OJXV</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>bethtexas</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>A clasically-styled and introverted album, Mem...</td>\n",
       "      <td>5</td>\n",
       "      <td>Enya at her most elegant</td>\n",
       "      <td>991526400</td>\n",
       "      <td>06 3, 2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A38IRL0X2T4DPF</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>bob turnley</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>I never thought Enya would reach the sublime h...</td>\n",
       "      <td>5</td>\n",
       "      <td>The best so far</td>\n",
       "      <td>1058140800</td>\n",
       "      <td>07 14, 2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A22IK3I6U76GX0</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>Calle</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This is the third review of an irish album I w...</td>\n",
       "      <td>5</td>\n",
       "      <td>Ireland produces good music.</td>\n",
       "      <td>957312000</td>\n",
       "      <td>05 3, 2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1AISPOIIHTHXX</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>Cloud \"...\"</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Enya, despite being a successful recording art...</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5; music to dream to</td>\n",
       "      <td>1200528000</td>\n",
       "      <td>01 17, 2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin          reviewerName helpful  \\\n",
       "0  A3EBHHCZO6V2A4  5555991584  Amaranth \"music fan\"  [3, 3]   \n",
       "1   AZPWAXJG9OJXV  5555991584             bethtexas  [0, 0]   \n",
       "2  A38IRL0X2T4DPF  5555991584           bob turnley  [2, 2]   \n",
       "3  A22IK3I6U76GX0  5555991584                 Calle  [1, 1]   \n",
       "4  A1AISPOIIHTHXX  5555991584           Cloud \"...\"  [1, 1]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  It's hard to believe \"Memory of Trees\" came ou...        5   \n",
       "1  A clasically-styled and introverted album, Mem...        5   \n",
       "2  I never thought Enya would reach the sublime h...        5   \n",
       "3  This is the third review of an irish album I w...        5   \n",
       "4  Enya, despite being a successful recording art...        4   \n",
       "\n",
       "                        summary  unixReviewTime   reviewTime  \n",
       "0       Enya's last great album      1158019200  09 12, 2006  \n",
       "1      Enya at her most elegant       991526400   06 3, 2001  \n",
       "2               The best so far      1058140800  07 14, 2003  \n",
       "3  Ireland produces good music.       957312000   05 3, 2000  \n",
       "4        4.5; music to dream to      1200528000  01 17, 2008  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"Digital_Music_5.json\", lines=True)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c3cdb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64706"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403b845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model...\n",
      "Encoding reviews with SBERT...\n",
      "Encoded 64705 interactions.\n",
      "Train: 51764, Val: 6470, Test: 6471\n",
      "Train samples: 51764, Val samples: 6470, Test samples: 6471\n",
      "Training model...\n",
      "Epoch 1: train MSE=2.7165, val MSE=1.0777\n",
      "Epoch 2: train MSE=0.9697, val MSE=1.0021\n",
      "Epoch 3: train MSE=0.9050, val MSE=0.9869\n",
      "Epoch 4: train MSE=0.8853, val MSE=0.9836\n",
      "Epoch 5: train MSE=0.8730, val MSE=0.9724\n",
      "Epoch 6: train MSE=0.8655, val MSE=0.9730\n",
      "Epoch 7: train MSE=0.8616, val MSE=0.9713\n",
      "Epoch 8: train MSE=0.8546, val MSE=0.9678\n",
      "Epoch 9: train MSE=0.8513, val MSE=0.9645\n",
      "Epoch 10: train MSE=0.8463, val MSE=0.9602\n",
      "Epoch 11: train MSE=0.8407, val MSE=0.9566\n",
      "Epoch 12: train MSE=0.8366, val MSE=0.9543\n",
      "Epoch 13: train MSE=0.8227, val MSE=0.9442\n",
      "Epoch 14: train MSE=0.8200, val MSE=0.9458\n",
      "Epoch 15: train MSE=0.8139, val MSE=0.9722\n",
      "Final TEST MSE: 0.9844\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "\n",
    "JSON_PATH = \"Digital_Music_5.json\"       \n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MAX_REVIEWS: Optional[int] = 100_000  \n",
    "TRAIN_FRACTION = 0.8\n",
    "VAL_FRACTION = 0.1\n",
    "SEED = 42\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 15\n",
    "LR = 1e-3\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data structures\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class Interaction:\n",
    "    user: str\n",
    "    item: str\n",
    "    rating: float\n",
    "    review_emb: np.ndarray  \n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SBERT\n",
    "# -----------------------------\n",
    "\n",
    "def build_sbert_model(model_name: str = MODEL_NAME) -> SentenceTransformer:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def encode_review(text: str, model: SentenceTransformer) -> np.ndarray:\n",
    "    sents = sent_tokenize(text)\n",
    "    if not sents:\n",
    "        return None\n",
    "    emb = model.encode(sents, convert_to_numpy=True)\n",
    "    return emb.mean(axis=0)\n",
    "\n",
    "def encode_reviews_with_sbert(\n",
    "    json_path: str,\n",
    "    model: SentenceTransformer,\n",
    "    max_reviews: Optional[int] = None,\n",
    ") -> List[Interaction]:\n",
    "    interactions: List[Interaction] = []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if max_reviews is not None and len(interactions) >= max_reviews:\n",
    "                break\n",
    "\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                r = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            # minimal required fields\n",
    "            if \"reviewerID\" not in r or \"asin\" not in r or \"overall\" not in r or \"reviewText\" not in r:\n",
    "                continue\n",
    "\n",
    "            text = r[\"reviewText\"]\n",
    "            emb = encode_review(text, model)\n",
    "            if emb is None:\n",
    "                continue\n",
    "\n",
    "            inter = Interaction(\n",
    "                user=str(r[\"reviewerID\"]),\n",
    "                item=str(r[\"asin\"]),\n",
    "                rating=float(r[\"overall\"]),\n",
    "                review_emb=emb,\n",
    "            )\n",
    "            interactions.append(inter)\n",
    "\n",
    "    return interactions\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Split\n",
    "# -----------------------------\n",
    "\n",
    "def split_interactions(\n",
    "    interactions: List[Interaction],\n",
    "    train_frac: float = TRAIN_FRACTION,\n",
    "    val_frac: float = VAL_FRACTION,\n",
    "    seed: int = SEED,\n",
    ") -> Tuple[List[Interaction], List[Interaction], List[Interaction]]:\n",
    "    rand = random.Random(seed)\n",
    "    shuffled = interactions[:]\n",
    "    rand.shuffle(shuffled)\n",
    "\n",
    "    n = len(shuffled)\n",
    "    n_train = int(n * train_frac)\n",
    "    n_val = int(n * val_frac)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train = shuffled[:n_train]\n",
    "    val = shuffled[n_train:n_train + n_val]\n",
    "    test = shuffled[n_train + n_val:]\n",
    "    assert len(train) + len(val) + len(test) == n\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Embedding Aggregation\n",
    "# -----------------------------\n",
    "\n",
    "def aggregate_embeddings(\n",
    "    interactions: List[Interaction],\n",
    ") -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n",
    "    user_sum: Dict[str, np.ndarray] = {}\n",
    "    user_count: Dict[str, int] = {}\n",
    "    item_sum: Dict[str, np.ndarray] = {}\n",
    "    item_count: Dict[str, int] = {}\n",
    "\n",
    "    for inter in interactions:\n",
    "        u, i, emb = inter.user, inter.item, inter.review_emb\n",
    "\n",
    "        # user\n",
    "        if u not in user_sum:\n",
    "            user_sum[u] = emb.copy()\n",
    "            user_count[u] = 1\n",
    "        else:\n",
    "            user_sum[u] += emb\n",
    "            user_count[u] += 1\n",
    "\n",
    "        # item\n",
    "        if i not in item_sum:\n",
    "            item_sum[i] = emb.copy()\n",
    "            item_count[i] = 1\n",
    "        else:\n",
    "            item_sum[i] += emb\n",
    "            item_count[i] += 1\n",
    "\n",
    "    user_vecs = {u: user_sum[u] / user_count[u] for u in user_sum}\n",
    "    item_vecs = {i: item_sum[i] / item_count[i] for i in item_sum}\n",
    "\n",
    "    return user_vecs, item_vecs\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PyTorch Dataset\n",
    "# -----------------------------\n",
    "\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        interactions: List[Interaction],\n",
    "        user_vecs: Dict[str, np.ndarray],\n",
    "        item_vecs: Dict[str, np.ndarray],\n",
    "    ):\n",
    "        samples_x = []\n",
    "        samples_y = []\n",
    "\n",
    "        for inter in interactions:\n",
    "            if inter.user not in user_vecs or inter.item not in item_vecs:\n",
    "                continue\n",
    "\n",
    "            u_vec = user_vecs[inter.user]\n",
    "            i_vec = item_vecs[inter.item]\n",
    "            x = np.concatenate([u_vec, i_vec], axis=0).astype(np.float32)\n",
    "            y = np.float32(inter.rating)\n",
    "\n",
    "            samples_x.append(x)\n",
    "            samples_y.append(y)\n",
    "\n",
    "        self.x = np.stack(samples_x) if samples_x else np.zeros((0,))\n",
    "        self.y = np.array(samples_y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.x[idx])\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Rating Model (MLP)\n",
    "# -----------------------------\n",
    "\n",
    "class RatingMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)  \n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    epochs: int = EPOCHS,\n",
    "    lr: float = LR,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_train_loss = total_loss / max(1, n_batches)\n",
    "\n",
    "        val_mse = evaluate_mse(model, val_loader, device=device)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train MSE={avg_train_loss:.4f}, val MSE={val_mse:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_mse(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, y)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    return total_loss / max(1, n_batches)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    print(\"Loading SBERT model...\")\n",
    "    sbert = build_sbert_model(MODEL_NAME)\n",
    "\n",
    "    print(\"Encoding reviews with SBERT...\")\n",
    "    interactions = encode_reviews_with_sbert(\n",
    "        JSON_PATH,\n",
    "        model=sbert,\n",
    "        max_reviews=MAX_REVIEWS,\n",
    "    )\n",
    "    print(f\"Encoded {len(interactions)} interactions.\")\n",
    "\n",
    "    if len(interactions) < 100:\n",
    "        print(\"Warning: very few interactions; consider increasing MAX_REVIEWS.\")\n",
    "\n",
    "    train, val, test = split_interactions(interactions)\n",
    "    print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
    "\n",
    "    user_vecs, item_vecs = aggregate_embeddings(train)\n",
    "    emb_dim = next(iter(user_vecs.values())).shape[0]\n",
    "    input_dim = emb_dim * 2\n",
    "\n",
    "    train_ds = RatingDataset(train, user_vecs, item_vecs)\n",
    "    val_ds = RatingDataset(val, user_vecs, item_vecs)\n",
    "    test_ds = RatingDataset(test, user_vecs, item_vecs)\n",
    "\n",
    "    print(\n",
    "        f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}, Test samples: {len(test_ds)}\"\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = RatingMLP(input_dim=input_dim)\n",
    "    print(\"Training model...\")\n",
    "    train_model(model, train_loader, val_loader)\n",
    "\n",
    "    test_mse = evaluate_mse(model, test_loader)\n",
    "    print(f\"Final TEST MSE: {test_mse:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
